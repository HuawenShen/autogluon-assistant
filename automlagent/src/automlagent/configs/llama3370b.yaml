# Tutorial Prompt Generator Configuration

stream_output: True
per_execution_timeout: 3600

max_chars_per_file: 1024
max_num_tutorials: 5
max_user_input_length: 2048
max_error_message_length: 2048
max_tutorial_length: 8192
create_venv: false
condense_tutorials: True

# Default LLM Configuration
# For each agent (coder, etc.) you can use a different one
llm: &default_llm
  # Note: bedrock is only supported in limited AWS regions
  #       and requires AWS credentials
  provider: bedrock
  model: "us.meta.llama3-3-70b-instruct-v1:0"
  # provider: openai
  # model: gpt-4o-2024-08-06
  max_tokens: 8192
  proxy_url: null
  temperature: 0
  verbose: True
  multi_turn: False

coder:
  <<: *default_llm  # Merge llm_config
  temperature: 0.5
  max_tokens: 8192
  top_p: 1
  multi_turn: False

planner:
  <<: *default_llm  # Merge llm_config
  multi_turn: False
  max_stdout_length: 8192
  max_stderr_length: 2048

file_reader:
  <<: *default_llm  # Merge llm_config
  multi_turn: False
  details: False
