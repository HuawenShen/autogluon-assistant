# Tutorial Prompt Generator Configuration

stream_output: True
per_execution_timeout: 3600

max_chars_per_file: 1024
max_num_tutorials: 5
max_user_input_length: 2048
max_error_message_length: 2048
max_tutorial_length: 8192
create_venv: false
condense_tutorials: True

# Default LLM Configuration
# For each agent (coder, etc.) you can use a different one
llm: &default_llm
  # Note: bedrock is only supported in limited AWS regions
  #       and requires AWS credentials
  provider: openai
  model: "o3-mini"
  # provider: openai
  # model: gpt-4o-2024-08-06
  max_tokens: 16384
  proxy_url: null
  verbose: True
  multi_turn: False

coder:
  <<: *default_llm  # Merge llm_config
  model: "o3-mini"
  max_tokens: 16384
  top_p: 1
  multi_turn: False
  add_coding_format_instruction: True

planner:
  <<: *default_llm  # Merge llm_config
  multi_turn: False
  max_stdout_length: 8192
  max_stderr_length: 2048

file_reader:
  <<: *default_llm  # Merge llm_config
  multi_turn: False
  details: False
